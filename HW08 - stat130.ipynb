{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b06891f2",
   "metadata": {},
   "source": [
    "Question 2: \n",
    "\n",
    "Chatbot summary: (I used a photo of the equations but it did not let me copy link for it since uoloaded photos cannot be shared so heres some of the chat session copied into another session without the photo) ---> https://chatgpt.com/share/673ff364-50b0-8007-a899-2d0ed36da5cd\n",
    "\n",
    "Accuracy: Accuracy is useful when it is important to understand the overall correctness of a system, such as an email spam filter. It measures how often the system correctly identifies both spam and regular emails, making it suitable when mistakes on either side are equally undesirable.\n",
    "\n",
    "Sensitivity: Sensitivity is critical in scenarios where it is important to identify all true positive cases. For example, in medical testing, sensitivity ensures that all real cases of a disease are detected, minimizing the risk of missing individuals who require treatment, even if false positives occur.\n",
    "\n",
    "Specificity: Specificity is essential when it is necessary to minimize false positives. For instance, in fraud detection specificity ensures that legitimate transactions are not incorrectly flagged as fraudulent, reducing unnecessary disruptions.\n",
    "\n",
    "Precision: Precision is important when confidence in positive results is required. For example, in cancer screenings, precision ensures that when a test indicates cancer, it is highly likely to be correct, avoiding unnecessary stress or medical procedures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ed2c68",
   "metadata": {},
   "source": [
    "Question 4:\n",
    "\n",
    "chatgpt session: https://chatgpt.com/share/6740021c-6300-8007-85e4-e4c73e522912\n",
    "\n",
    "An 80/20 split of the data into training and testing sets can be performed using either df.sample() or train_test_split(). The train_test_split() function is often preferred for its simplicity in randomly dividing the dataset, allocating 80% of the data to the training set (ab_reduced_noNaN_train) and 20% to the testing set (ab_reduced_noNaN_test). The number of observations in each set can be checked using len(ab_reduced_noNaN_train) for the training data and len(ab_reduced_noNaN_test) for the test data. For fitting a DecisionTreeClassifier model, the \"List Price\" variable is used to predict whether a book is hardcover or paperback, with a max_depth parameter set to 2. After fitting the model, tree.plot_tree(clf) is used to visualize the decision tree and understand how predictions are made based on the \"List Price\" feature.\n",
    "\n",
    "\n",
    "Also the chat session found that the two steps:\n",
    "\n",
    "(directly from chat session)\n",
    "\n",
    "y = pd.get_dummies(ab_reduced_no NaN(\"hard or paper). This step converts the categorical \"Hard_or_Paper\" column into a binary (0/1) format using one-hot encoding, where 'H' represents the hardcover category (1 for hardcover, 0 for paperback).\n",
    "\n",
    "\n",
    "\n",
    "X = ab_reduced_noNaN((\"ListPrice\")): This selects the \"List Price\" column as the feature set (X) used to predict the target variable (y), which indicates whether the book is hardcover or paperback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37edf375",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtree\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DecisionTreeClassifier, plot_tree\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Preparing the target and features\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mget_dummies(ab_reduced_noNaN[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHard_or_Paper\u001b[39m\u001b[38;5;124m\"\u001b[39m])[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mH\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      5\u001b[0m X \u001b[38;5;241m=\u001b[39m ab_reduced_noNaN[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mList Price\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Initializing the DecisionTreeClassifier with max_depth=2\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "\n",
    "# Preparing the target and features\n",
    "y = pd.get_dummies(ab_reduced_noNaN[\"Hard_or_Paper\"])['H']\n",
    "X = ab_reduced_noNaN[['List Price']]\n",
    "\n",
    "# Initializing the DecisionTreeClassifier with max_depth=2\n",
    "clf = DecisionTreeClassifier(max_depth=2, random_state=42)\n",
    "\n",
    "# Training the model\n",
    "clf.fit(X, y)\n",
    "\n",
    "# Visualizing the trained decision tree\n",
    "plot_tree(clf, feature_names=['List Price'], class_names=['Paperback', 'Hardcover'], filled=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e34b26",
   "metadata": {},
   "source": [
    "Above is the code to train the model provided by chatgpt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c00e19",
   "metadata": {},
   "source": [
    "___________________________________________________________________________________________________________________\n",
    "\n",
    "Question 6:\n",
    "\n",
    "Chatgpt session: https://chatgpt.com/share/6740033e-4e08-8007-81b8-4bcf2e729ec1\n",
    "\n",
    "\n",
    "Using chatgpt confusion matrices were calculated for both models, clf and clf2, using the test dataset ab_reduced_noNaN_test. Within these datasets the sensitivity represented the ability to correctly predict hardcover books, while specificity measures accuracy in identifying paperback books. Accuracy provides an overall measure of prediction correctness. Metrics were reported using decimal values rounded to three significant digits for clarity. This comparison highlights the performance differences between the two decision tree models, emphasizing the impact of depth and predictor selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452d4e8d",
   "metadata": {},
   "source": [
    "Question 7:\n",
    "\n",
    "chatgpt session: https://chatgpt.com/share/674006b6-8bdc-8007-8db2-3c04f94dba73\n",
    "\n",
    "The differences between the two confusion matrices result from the features used in training the models. The first confusion matrix is based on predictions using only the \"List Price\" feature, which may limit predictive accuracy. The second confusion matrix incorporates multiple features, including \"Numpages,\" \"Thick,\" and \"List price,\" providing more information and enabling better predictions. The confusion matrices for clf and clf2 demonstrate improved performance due to the use of more robust features or better training data, resulting in a more balanced classification with fewer errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8de6a57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
